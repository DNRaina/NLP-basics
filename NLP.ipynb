{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1hgeppKkxJB",
        "outputId": "151c723a-5d20-46d4-f04f-20acd7936fa5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wpwv-H2h8CvK"
      },
      "outputs": [],
      "source": [
        "def sentenceSegment(text):\n",
        "    sentences = []\n",
        "    start = 0\n",
        "\n",
        "    for i in range(len(text)):\n",
        "        if text[i] in ('.', '!', '?'):\n",
        "            sentence = text[start:i + 1].strip()\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "            start = i + 1\n",
        "\n",
        "    # Add the last sentence if any\n",
        "    last_sentence = text[start:].strip()\n",
        "    if last_sentence:\n",
        "        sentences.append(last_sentence)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "\n",
        "def remove_punctuation(input_string):\n",
        "    # Define a string of punctuation marks and symbols\n",
        "    punctuations = string.punctuation\n",
        "\n",
        "    # Remove the punctuation marks and symbols from the input string\n",
        "    output_string = \"\".join(char for char in input_string if char not in punctuations)\n",
        "\n",
        "    return output_string\n",
        "\n",
        "def convertToLower(s):\n",
        "  return s.lower()\n",
        "\n",
        "def stopWordRemoval(words):\n",
        "    j = 0\n",
        "    while (j < len(words)):\n",
        "        if(len(words[j]) < 3):\n",
        "            words.remove(words[j])\n",
        "        else:\n",
        "            j = j + 1\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define stop words\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "AORBatawojBc"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"the original text is\")\n",
        "text = \"HThank you Vandana mam for the guidance, please wish me more luck !\"\n",
        "print(text)\n",
        "\n",
        "puncRemovedText = remove_punctuation(text)\n",
        "print(\"\\n the punctuation removed text is\")\n",
        "print(puncRemovedText +\"\\n\")\n",
        "\n",
        "lowerText = convertToLower(puncRemovedText)\n",
        "print(\"text is converted to lower now\")\n",
        "print(lowerText +\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "If7-UDxEp1HK",
        "outputId": "b25abdb7-b96e-4cf2-ae83-873715a375ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the original text is\n",
            "HThank you Vandana mam for the guidance, please wish me more luck !\n",
            "\n",
            " the punctuation removed text is\n",
            "HThank you Vandana mam for the guidance please wish me more luck \n",
            "\n",
            "text is converted to lower now\n",
            "hthank you vandana mam for the guidance please wish me more luck \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sentenceSegment(lowerText)\n",
        "print(sentences)\n",
        "\n",
        "tokens = nltk.word_tokenize(lowerText)\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
        "\n",
        "# Perform stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "# Print the stemmed tokens\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wbwm1HXpm0f",
        "outputId": "b7e9461f-b8c7-48ce-fee2-891844244e17"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hthank you vandana mam for the guidance please wish me more luck']\n",
            "['hthank', 'vandana', 'mam', 'guidanc', 'pleas', 'wish', 'luck']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Assuming 'tokenized_and_cleaned_text' is a list of preprocessed text strings\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(stemmed_tokens)"
      ],
      "metadata": {
        "id": "_LF9s3bEDRXb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlteUGVLDW5A",
        "outputId": "d98cc3c3-f38d-4f89-dda7-fc1d241a5fd1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1\n",
            "  (1, 5)\t1\n",
            "  (2, 3)\t1\n",
            "  (3, 0)\t1\n",
            "  (4, 4)\t1\n",
            "  (5, 6)\t1\n",
            "  (6, 2)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Assuming 'tokenized_and_cleaned_text' is a list of preprocessed text strings\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(stemmed_tokens)\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuIVbt1lDbH9",
        "outputId": "4f4cde7f-4371-4fa1-8cda-5bf6b99fd725"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1)\t1.0\n",
            "  (1, 5)\t1.0\n",
            "  (2, 3)\t1.0\n",
            "  (3, 0)\t1.0\n",
            "  (4, 4)\t1.0\n",
            "  (5, 6)\t1.0\n",
            "  (6, 2)\t1.0\n"
          ]
        }
      ]
    }
  ]
}